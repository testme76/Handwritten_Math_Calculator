{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU device: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Define device first\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = str('alexli76')\n",
    "key = str('01b753e09563c67314e7c90d9fd4f6a8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil  # Add this import\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import time\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type bytes is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m kaggle_token \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m: username,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m: key\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/.kaggle/kaggle.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkaggle_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Set permissions\u001b[39;00m\n\u001b[0;32m     18\u001b[0m os\u001b[38;5;241m.\u001b[39mchmod(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/.kaggle/kaggle.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0o600\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alexl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alexl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type bytes is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Download dataset from Kaggle\n",
    "import opendatasets as od\n",
    "import pandas\n",
    "\n",
    "# Create .kaggle directory if it doesn't exist\n",
    "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    kaggle_token = {\n",
    "        \"username\": username,\n",
    "        \"key\": key\n",
    "    }\n",
    "    \n",
    "    # Verify the data is string\n",
    "    print(\"\\nVerifying credentials format:\")\n",
    "    print(f\"Username type: {type(username)}\")\n",
    "    print(f\"Key type: {type(key)}\")\n",
    "    \n",
    "    with open(os.path.expanduser('~/.kaggle/kaggle.json'), 'w') as f:\n",
    "        json.dump(kaggle_token, f)\n",
    "    \n",
    "    print(\"Kaggle credentials saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Kaggle credentials: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set permissions\n",
    "os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n",
    "\n",
    "# Download dataset\n",
    "dataset_url = \"https://www.kaggle.com/datasets/sagyamthapa/handwritten-math-symbols\"\n",
    "od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting extraction process...\n",
      "Looking for archive at: ./handwritten-math-symbols.zip\n",
      "Archive exists: False\n",
      "\n",
      "Starting file organization...\n",
      "Looking for source directory at: ./handwritten-math-symbols/dataset\n",
      "Source directory exists: True\n",
      "\n",
      "Moving digit folders...\n",
      "Copying ./handwritten-math-symbols/dataset\\0 to ./data/digits\\0\n",
      "Copying ./handwritten-math-symbols/dataset\\1 to ./data/digits\\1\n",
      "Copying ./handwritten-math-symbols/dataset\\2 to ./data/digits\\2\n",
      "Copying ./handwritten-math-symbols/dataset\\3 to ./data/digits\\3\n",
      "Copying ./handwritten-math-symbols/dataset\\4 to ./data/digits\\4\n",
      "Copying ./handwritten-math-symbols/dataset\\5 to ./data/digits\\5\n",
      "Copying ./handwritten-math-symbols/dataset\\6 to ./data/digits\\6\n",
      "Copying ./handwritten-math-symbols/dataset\\7 to ./data/digits\\7\n",
      "Copying ./handwritten-math-symbols/dataset\\8 to ./data/digits\\8\n",
      "Copying ./handwritten-math-symbols/dataset\\9 to ./data/digits\\9\n",
      "\n",
      "Moving operator folders...\n",
      "Copying ./handwritten-math-symbols/dataset\\add to ./data/operators\\add\n",
      "Copying ./handwritten-math-symbols/dataset\\sub to ./data/operators\\sub\n",
      "Copying ./handwritten-math-symbols/dataset\\mul to ./data/operators\\mul\n",
      "Copying ./handwritten-math-symbols/dataset\\div to ./data/operators\\div\n",
      "Copying ./handwritten-math-symbols/dataset\\eq to ./data/operators\\eq\n",
      "Copying ./handwritten-math-symbols/dataset\\dec to ./data/operators\\dec\n",
      "Copying ./handwritten-math-symbols/dataset\\x to ./data/operators\\x\n",
      "Copying ./handwritten-math-symbols/dataset\\y to ./data/operators\\y\n",
      "Copying ./handwritten-math-symbols/dataset\\z to ./data/operators\\z\n",
      "\n",
      "Verifying directory structure:\n",
      "\n",
      "Directory: ./data\n",
      "Subdirectories: ['digits', 'operators']\n",
      "Number of files: 0\n",
      "\n",
      "Directory: ./data\\digits\n",
      "Subdirectories: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Number of files: 0\n",
      "\n",
      "Directory: ./data\\digits\\0\n",
      "Subdirectories: []\n",
      "Number of files: 595\n",
      "\n",
      "Directory: ./data\\digits\\1\n",
      "Subdirectories: []\n",
      "Number of files: 562\n",
      "\n",
      "Directory: ./data\\digits\\2\n",
      "Subdirectories: []\n",
      "Number of files: 433\n",
      "\n",
      "Directory: ./data\\digits\\3\n",
      "Subdirectories: []\n",
      "Number of files: 541\n",
      "\n",
      "Directory: ./data\\digits\\4\n",
      "Subdirectories: []\n",
      "Number of files: 526\n",
      "\n",
      "Directory: ./data\\digits\\5\n",
      "Subdirectories: []\n",
      "Number of files: 433\n",
      "\n",
      "Directory: ./data\\digits\\6\n",
      "Subdirectories: []\n",
      "Number of files: 581\n",
      "\n",
      "Directory: ./data\\digits\\7\n",
      "Subdirectories: []\n",
      "Number of files: 533\n",
      "\n",
      "Directory: ./data\\digits\\8\n",
      "Subdirectories: []\n",
      "Number of files: 554\n",
      "\n",
      "Directory: ./data\\digits\\9\n",
      "Subdirectories: []\n",
      "Number of files: 547\n",
      "\n",
      "Directory: ./data\\operators\n",
      "Subdirectories: ['add', 'dec', 'div', 'eq', 'mul', 'sub', 'x', 'y', 'z']\n",
      "Number of files: 0\n",
      "\n",
      "Directory: ./data\\operators\\add\n",
      "Subdirectories: []\n",
      "Number of files: 596\n",
      "\n",
      "Directory: ./data\\operators\\dec\n",
      "Subdirectories: []\n",
      "Number of files: 624\n",
      "\n",
      "Directory: ./data\\operators\\div\n",
      "Subdirectories: []\n",
      "Number of files: 618\n",
      "\n",
      "Directory: ./data\\operators\\eq\n",
      "Subdirectories: []\n",
      "Number of files: 634\n",
      "\n",
      "Directory: ./data\\operators\\mul\n",
      "Subdirectories: []\n",
      "Number of files: 577\n",
      "\n",
      "Directory: ./data\\operators\\sub\n",
      "Subdirectories: []\n",
      "Number of files: 655\n",
      "\n",
      "Directory: ./data\\operators\\x\n",
      "Subdirectories: []\n",
      "Number of files: 452\n",
      "\n",
      "Directory: ./data\\operators\\y\n",
      "Subdirectories: []\n",
      "Number of files: 399\n",
      "\n",
      "Directory: ./data\\operators\\z\n",
      "Subdirectories: []\n",
      "Number of files: 212\n",
      "\n",
      "Verifying data in folders:\n",
      "\n",
      "Found digit classes: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Found operator classes: ['add', 'dec', 'div', 'eq', 'mul', 'sub', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Extract dataset from archive with more detailed logging\n",
    "print(\"\\nStarting extraction process...\")\n",
    "dataset_path = \"./handwritten-math-symbols\"\n",
    "archive_path = f\"{dataset_path}.zip\"\n",
    "print(f\"Looking for archive at: {archive_path}\")\n",
    "print(f\"Archive exists: {os.path.exists(archive_path)}\")\n",
    "\n",
    "if os.path.exists(archive_path):\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "        # List contents of zip file\n",
    "        print(\"\\nContents of zip file:\")\n",
    "        for file in zip_ref.namelist()[:10]:  # Show first 10 files\n",
    "            print(f\"- {file}\")\n",
    "        print(\"...\")\n",
    "        \n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Dataset extracted successfully\")\n",
    "\n",
    "def verify_directory_structure():\n",
    "    \"\"\"Verify and print the directory structure\"\"\"\n",
    "    print(\"\\nVerifying directory structure:\")\n",
    "    for root, dirs, files in os.walk('./data'):\n",
    "        print(f\"\\nDirectory: {root}\")\n",
    "        print(f\"Subdirectories: {dirs}\")\n",
    "        print(f\"Number of files: {len(files)}\")\n",
    "\n",
    "# First organize the data\n",
    "print(\"\\nStarting file organization...\")\n",
    "source_dir = \"./handwritten-math-symbols/dataset\"\n",
    "print(f\"Looking for source directory at: {source_dir}\")\n",
    "print(f\"Source directory exists: {os.path.exists(source_dir)}\")\n",
    "\n",
    "if not os.path.exists(source_dir):\n",
    "    raise FileNotFoundError(f\"Source directory {source_dir} not found!\")\n",
    "\n",
    "# Create destination directories\n",
    "os.makedirs('./data/digits', exist_ok=True)\n",
    "os.makedirs('./data/operators', exist_ok=True)\n",
    "\n",
    "# Define which folders belong to digits and operators\n",
    "digit_folders = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "operator_folders = ['add', 'sub', 'mul', 'div', 'eq', 'dec', 'x', 'y', 'z']\n",
    "\n",
    "# Move digit folders\n",
    "print(\"\\nMoving digit folders...\")\n",
    "for folder in digit_folders:\n",
    "    src = os.path.join(source_dir, folder)\n",
    "    dst = os.path.join('./data/digits', folder)\n",
    "    if os.path.exists(src):\n",
    "        print(f\"Copying {src} to {dst}\")\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    else:\n",
    "        print(f\"Warning: Source folder not found: {src}\")\n",
    "\n",
    "# Move operator folders\n",
    "print(\"\\nMoving operator folders...\")\n",
    "for folder in operator_folders:\n",
    "    src = os.path.join(source_dir, folder)\n",
    "    dst = os.path.join('./data/operators', folder)\n",
    "    if os.path.exists(src):\n",
    "        print(f\"Copying {src} to {dst}\")\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    else:\n",
    "        print(f\"Warning: Source folder not found: {src}\")\n",
    "\n",
    "# Verify the directory structure\n",
    "verify_directory_structure()\n",
    "\n",
    "# Verify that we have data in the folders\n",
    "print(\"\\nVerifying data in folders:\")\n",
    "digit_path = './data/digits'\n",
    "operator_path = './data/operators'\n",
    "\n",
    "if not os.path.exists(digit_path) or not os.path.exists(operator_path):\n",
    "    raise FileNotFoundError(\"Data directories not created properly!\")\n",
    "\n",
    "digit_classes = sorted(os.listdir(digit_path))\n",
    "operator_classes = sorted(os.listdir(operator_path))\n",
    "\n",
    "print(f\"\\nFound digit classes: {digit_classes}\")\n",
    "print(f\"Found operator classes: {operator_classes}\")\n",
    "\n",
    "if not digit_classes:\n",
    "    raise FileNotFoundError(\"No digit classes found!\")\n",
    "if not operator_classes:\n",
    "    raise FileNotFoundError(\"No operator classes found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Digit dataset created successfully with 5304 images\n",
      "Operator dataset created successfully with 4767 images\n",
      "\n",
      "Datasets and dataloaders created successfully!\n",
      "Number of digit classes: 10\n",
      "Number of operator classes: 9\n"
     ]
    }
   ],
   "source": [
    "# Create transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Create datasets with error handling\n",
    "try:\n",
    "    digit_dataset = torchvision.datasets.ImageFolder(\n",
    "        root='./data/digits',\n",
    "        transform=transform\n",
    "    )\n",
    "    print(f\"\\nDigit dataset created successfully with {len(digit_dataset)} images\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating digit dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    operator_dataset = torchvision.datasets.ImageFolder(\n",
    "        root='./data/operators',\n",
    "        transform=transform\n",
    "    )\n",
    "    print(f\"Operator dataset created successfully with {len(operator_dataset)} images\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating operator dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create dataloaders\n",
    "digit_trainloader = torch.utils.data.DataLoader(\n",
    "    digit_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "operator_trainloader = torch.utils.data.DataLoader(\n",
    "    operator_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "print(\"\\nDatasets and dataloaders created successfully!\")\n",
    "print(f\"Number of digit classes: {len(digit_dataset.classes)}\")\n",
    "print(f\"Number of operator classes: {len(operator_dataset.classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnostic Information:\n",
      "Digit classes: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Number of digit classes: 10\n",
      "DigitNet output dimension: 10\n",
      "Operator classes: ['add', 'dec', 'div', 'eq', 'mul', 'sub', 'x', 'y', 'z']\n",
      "Number of operator classes: 9\n",
      "OperatorNet output dimension: 9\n",
      "\n",
      "Network Architecture Check:\n",
      "DigitNet:\n",
      "Input -> Conv1 (1->6) -> Pool -> Conv2 (6->16) -> Pool -> FC1 (400->120) -> FC2 (120->84) -> FC3 (84->10)\n",
      "\n",
      "OperatorNet:\n",
      "Input -> Conv1 (1->6) -> Pool -> Conv2 (6->16) -> Pool -> FC1 (400->120) -> FC2 (120->84) -> FC3 (84->9)\n",
      "\n",
      "Moving networks to GPU...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMoving networks to GPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     digit_net \u001b[38;5;241m=\u001b[39m digit_net\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[0;32m     73\u001b[0m     operator_net \u001b[38;5;241m=\u001b[39m operator_net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully moved networks to GPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the classes based on the folders\n",
    "digit_classes = sorted(os.listdir('./data/digits'))\n",
    "operator_classes = sorted(os.listdir('./data/operators'))\n",
    "\n",
    "# Define network architectures first\n",
    "class DigitNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitNet, self).__init__()\n",
    "        # Input: 1x32x32\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # Output: 6x28x28\n",
    "        self.pool = nn.MaxPool2d(2, 2)    # Output: 6x14x14\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Output: 16x10x10\n",
    "        # After second pooling: 16x5x5\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, len(digit_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class OperatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OperatorNet, self).__init__()\n",
    "        # Input: 1x32x32\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # Output: 6x28x28\n",
    "        self.pool = nn.MaxPool2d(2, 2)    # Output: 6x14x14\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Output: 16x10x10\n",
    "        # After second pooling: 16x5x5\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, len(operator_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize networks on CPU first\n",
    "digit_net = DigitNet()\n",
    "operator_net = OperatorNet()\n",
    "\n",
    "# Diagnostic code to check dimensions and classes\n",
    "print(\"\\nDiagnostic Information:\")\n",
    "print(f\"Digit classes: {digit_classes}\")\n",
    "print(f\"Number of digit classes: {len(digit_classes)}\")\n",
    "print(f\"DigitNet output dimension: {digit_net.fc3.out_features}\")\n",
    "\n",
    "print(f\"Operator classes: {operator_classes}\")\n",
    "print(f\"Number of operator classes: {len(operator_classes)}\")\n",
    "print(f\"OperatorNet output dimension: {operator_net.fc3.out_features}\")\n",
    "\n",
    "# Verify network structures before moving to GPU\n",
    "print(\"\\nNetwork Architecture Check:\")\n",
    "print(\"DigitNet:\")\n",
    "print(f\"Input -> Conv1 (1->6) -> Pool -> Conv2 (6->16) -> Pool -> FC1 (400->120) -> FC2 (120->84) -> FC3 (84->{len(digit_classes)})\")\n",
    "print(\"\\nOperatorNet:\")\n",
    "print(f\"Input -> Conv1 (1->6) -> Pool -> Conv2 (6->16) -> Pool -> FC1 (400->120) -> FC2 (120->84) -> FC3 (84->{len(operator_classes)})\")\n",
    "\n",
    "# Try moving to GPU with error handling\n",
    "try:\n",
    "    print(\"\\nMoving networks to GPU...\")\n",
    "    digit_net = digit_net.to(device)\n",
    "    operator_net = operator_net.to(device)\n",
    "    print(\"Successfully moved networks to GPU\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error moving networks to GPU: {e}\")\n",
    "    print(\"Falling back to CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    digit_net = digit_net.to(device)\n",
    "    operator_net = operator_net.to(device)\n",
    "\n",
    "# Create optimizers after moving networks to device\n",
    "digit_optimizer = optim.SGD(digit_net.parameters(), lr=0.001, momentum=0.9)\n",
    "operator_optimizer = optim.SGD(operator_net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Now verify data and label shapes\n",
    "print(\"\\nVerifying data shapes:\")\n",
    "sample_digit_batch = next(iter(digit_trainloader))\n",
    "sample_operator_batch = next(iter(operator_trainloader))\n",
    "\n",
    "print(f\"Digit batch - Images: {sample_digit_batch[0].shape}, Labels: {sample_digit_batch[1].shape}\")\n",
    "print(f\"Operator batch - Images: {sample_operator_batch[0].shape}, Labels: {sample_operator_batch[1].shape}\")\n",
    "\n",
    "# Check label distributions\n",
    "print(\"\\nLabel distributions:\")\n",
    "print(f\"Digit labels unique values: {torch.unique(sample_digit_batch[1])}\")\n",
    "print(f\"Operator labels unique values: {torch.unique(sample_operator_batch[1])}\")\n",
    "\n",
    "# Verify network output dimensions match number of classes\n",
    "with torch.no_grad():\n",
    "    digit_out = digit_net(sample_digit_batch[0].to(device))\n",
    "    operator_out = operator_net(sample_operator_batch[0].to(device))\n",
    "    print(\"\\nNetwork output dimensions:\")\n",
    "    print(f\"DigitNet output: {digit_out.shape}\")\n",
    "    print(f\"OperatorNet output: {operator_out.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m epochs_without_improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get current date for logging\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m training_date \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Add learning rate scheduler\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 50\n",
    "eval_interval = 5\n",
    "early_stopping_patience = 15\n",
    "best_digit_accuracy = 0\n",
    "best_operator_accuracy = 0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Get current date for logging\n",
    "training_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Add learning rate scheduler\n",
    "digit_scheduler = ReduceLROnPlateau(digit_optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "operator_scheduler = ReduceLROnPlateau(operator_optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Diagnostic code to check dimensions and classes\n",
    "print(\"\\nDiagnostic Information:\")\n",
    "\n",
    "# Check digit dataset\n",
    "digit_classes = sorted(os.listdir('./data/digits'))\n",
    "print(f\"Digit classes: {digit_classes}\")\n",
    "print(f\"Number of digit classes: {len(digit_classes)}\")\n",
    "print(f\"DigitNet output dimension: {digit_net.fc3.out_features}\")\n",
    "\n",
    "# Check operator dataset\n",
    "operator_classes = sorted(os.listdir('./data/operators'))\n",
    "print(f\"Operator classes: {operator_classes}\")\n",
    "print(f\"Number of operator classes: {len(operator_classes)}\")\n",
    "print(f\"OperatorNet output dimension: {operator_net.fc3.out_features}\")\n",
    "\n",
    "# Check a batch of data\n",
    "digit_batch = next(iter(digit_trainloader))\n",
    "operator_batch = next(iter(operator_trainloader))\n",
    "\n",
    "print(\"\\nBatch shapes:\")\n",
    "print(f\"Digit batch - Images: {digit_batch[0].shape}, Labels: {digit_batch[1].shape}\")\n",
    "print(f\"Operator batch - Images: {operator_batch[0].shape}, Labels: {operator_batch[1].shape}\")\n",
    "\n",
    "print(\"\\nLabel ranges:\")\n",
    "print(f\"Digit labels: {digit_batch[1].min().item()} to {digit_batch[1].max().item()}\")\n",
    "print(f\"Operator labels: {operator_batch[1].min().item()} to {operator_batch[1].max().item()}\")\n",
    "\n",
    "\n",
    "# Add criterion definitions before training\n",
    "digit_criterion = nn.CrossEntropyLoss()\n",
    "operator_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Add dataset class mapping\n",
    "digit_dataset.class_to_idx  # Check the mapping of classes to indices\n",
    "operator_dataset.class_to_idx  # Check the mapping of classes to indices\n",
    "\n",
    "# Before training loop, add diagnostic prints\n",
    "print(\"\\nClass mappings:\")\n",
    "print(f\"Digit classes: {digit_dataset.class_to_idx}\")\n",
    "print(f\"Operator classes: {operator_dataset.class_to_idx}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training digit network\n",
    "    digit_net.train()\n",
    "    digit_running_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Add progress bar\n",
    "    pbar = tqdm(digit_trainloader, desc=f\"Training Digit Net\")\n",
    "    for i, data in enumerate(pbar):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        digit_optimizer.zero_grad()\n",
    "        outputs = digit_net(inputs)\n",
    "        loss = digit_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        digit_optimizer.step()\n",
    "        \n",
    "        digit_running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            pbar.set_postfix({'loss': digit_running_loss/100})\n",
    "            digit_running_loss = 0.0\n",
    "    \n",
    "    # Training operator network\n",
    "    operator_net.train()\n",
    "    operator_running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(operator_trainloader, desc=f\"Training Operator Net\")\n",
    "    for i, data in enumerate(pbar):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        operator_optimizer.zero_grad()\n",
    "        outputs = operator_net(inputs)\n",
    "        loss = operator_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        operator_optimizer.step()\n",
    "        \n",
    "        operator_running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            pbar.set_postfix({'loss': operator_running_loss/100})\n",
    "            operator_running_loss = 0.0\n",
    "    \n",
    "    # Evaluation every eval_interval epochs\n",
    "    if epoch % eval_interval == 0:\n",
    "        digit_net.eval()\n",
    "        operator_net.eval()\n",
    "        \n",
    "        # Evaluate digit network\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in digit_trainloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = digit_net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        current_digit_accuracy = 100 * correct / total\n",
    "        print(f\"\\nDigit Network Accuracy: {current_digit_accuracy:.2f}%\")\n",
    "        \n",
    "        # Evaluate operator network\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in operator_trainloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = operator_net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        current_operator_accuracy = 100 * correct / total\n",
    "        print(f\"Operator Network Accuracy: {current_operator_accuracy:.2f}%\")\n",
    "        \n",
    "        # Update schedulers\n",
    "        digit_scheduler.step(current_digit_accuracy)\n",
    "        operator_scheduler.step(current_operator_accuracy)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if current_digit_accuracy > best_digit_accuracy or current_operator_accuracy > best_operator_accuracy:\n",
    "            best_digit_accuracy = max(best_digit_accuracy, current_digit_accuracy)\n",
    "            best_operator_accuracy = max(best_operator_accuracy, current_operator_accuracy)\n",
    "            torch.save(digit_net.state_dict(), 'digit_net_best.pth')\n",
    "            torch.save(operator_net.state_dict(), 'operator_net_best.pth')\n",
    "            epochs_without_improvement = 0\n",
    "            print(\"New best accuracy! Saved models.\")\n",
    "        else:\n",
    "            epochs_without_improvement += eval_interval\n",
    "        \n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"\\nEarly stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Print epoch summary\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Time taken: {epoch_time:.2f} seconds\")\n",
    "    print(f\"Best Digit Accuracy: {best_digit_accuracy:.2f}%\")\n",
    "    print(f\"Best Operator Accuracy: {best_operator_accuracy:.2f}%\")\n",
    "\n",
    "print('Finished Training')\n",
    "# Save final weights\n",
    "print(\"\\nSaving final model weights...\")\n",
    "try:\n",
    "    final_digit_path = 'digit_net_final.pth'\n",
    "    final_operator_path = 'operator_net_final.pth'\n",
    "    \n",
    "    torch.save(digit_net.state_dict(), final_digit_path)\n",
    "    torch.save(operator_net.state_dict(), final_operator_path)\n",
    "    \n",
    "    # Verify files were saved\n",
    "    if os.path.exists(final_digit_path) and os.path.exists(final_operator_path):\n",
    "        print(f\"Final weights saved successfully:\")\n",
    "        print(f\"- Digit network: {final_digit_path}\")\n",
    "        print(f\"- Operator network: {final_operator_path}\")\n",
    "    else:\n",
    "        print(\"Warning: Weight files not found after saving!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final weights: {e}\")\n",
    "\n",
    "# Also verify best weights were saved\n",
    "best_digit_path = 'digit_net_best.pth'\n",
    "best_operator_path = 'operator_net_best.pth'\n",
    "\n",
    "if os.path.exists(best_digit_path) and os.path.exists(best_operator_path):\n",
    "    print(\"\\nBest weights were saved during training:\")\n",
    "    print(f\"- Best digit network: {best_digit_path}\")\n",
    "    print(f\"- Best operator network: {best_operator_path}\")\n",
    "else:\n",
    "    print(\"\\nWarning: Best weight files not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
